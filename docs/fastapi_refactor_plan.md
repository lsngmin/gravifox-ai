# FastAPI 서버 및 모델 추론 파이프라인 개선 아이디어 10선

## 1. 비동기 배치 추론 큐 도입
- **배경**: 현재 추론 엔드포인트는 요청마다 즉시 모델을 호출한다고 가정돼요. 대형 모델에서 TPS가 증가하면 GPU 컨텍스트 전환과 I/O 대기 때문에 처리량이 급락해요.
- **개선안**: `core/infer/predict.py` 호출을 중앙 큐(`asyncio.Queue`)로 집계하고, 백그라운드 워커가 일정 주기로 배치 추론을 실행하도록 리팩터링해요. FastAPI `BackgroundTasks` 또는 `Depends` 기반 의존성으로 큐 핸들러를 주입하면 돼요.
- **기대효과**: GPU 활용률 향상(동시 요청 8~16건을 묶어 처리), 응답 지연 표준편차 축소, 추론 중복 실행 방지.

## 2. 모델 로딩/캐시 관리 레이어 분리
- **배경**: 모델 체크포인트 로딩 로직이 엔드포인트에 분산되어 있으면 코드 중복과 warm-start 지연이 발생해요.
- **개선안**: `core/infer/registry.py` 스타일의 모델 팩토리를 도입하고, 애플리케이션 시작 시 `lifespan` 이벤트에서 모든 필요 모델을 로드해 캐시해요. Hot reload를 위해 파일시스템 watcher와 함께 `model_version` 메타데이터를 관리해요.
- **기대효과**: cold-start 시간을 크게 줄이고, 다중 모델 버전에 대한 A/B 테스트 및 롤백이 쉬워져요.

## 3. 추론 파이프라인 단계별 로깅/관측성 강화
- **배경**: 현재 로그가 부족하면 장애 분석이 어려워요.
- **개선안**: `core/utils/logger.py`의 `log_time` 데코레이터로 전처리→모델추론→후처리 단계별 시간을 계측하고, `structlog`/`OpenTelemetry` 연동으로 추적 데이터를 수집해요. 사용자 입력 요약, 추론 성공/실패, latency histogram을 Metrics로 내보내요.
- **기대효과**: SLA 위반 조기 감지, 장애 RCA 속도 향상, APM 연동 기반 자동 스케일링 신뢰도 개선.

## 4. 동적 임계값 & 품질 추정 모듈화
- **배경**: SNS 재인코딩으로 인해 confidence 분포가 흔들리면 고정 임계값이 부적절해요.
- **개선안**: `core/infer/calibration.py` 모듈을 만들어 품질 점수(Q/σ)와 Temperature Scaling 파라미터를 함께 관리하고, 요청마다 adaptive threshold를 계산하도록 분리해요.
- **기대효과**: 재업로드 요청 구간(0.45~0.55)에 대한 일관된 UX 유지, 오탐/미탐률 감소.

## 5. 입력 검증 & 스트림 처리 강화
- **배경**: 대용량 파일이나 악성 입력을 그대로 받아들이면 서버 자원이 고갈될 수 있어요.
- **개선안**: Pydantic v2 모델을 활용해 파일 크기, 포맷, 프레임 길이 제한을 명시하고, `StreamingResponse` 기반 chunked 읽기를 적용해요. 영상은 `ffmpeg` 프로빙 후 샘플링, 이미지는 Pillow `Image.open`으로 lazy 검증해요.
- **기대효과**: DoS 방어선 강화, 메모리/CPU 스파이크 완화, 예측 가능한 에러 응답 제공.

## 6. 리얼-타임 모니터링을 위한 웹소켓 엔드포인트 추가
- **배경**: 대량 업로드나 관리자 대시보드에서 실시간 추론 상태를 확인하기 어려워요.
- **개선안**: FastAPI WebSocket 엔드포인트(`/ws/predict-status`)를 추가하고, 배치 큐 진행 상태 및 confidence 업데이트를 push해요. Redis Pub/Sub 또는 `asyncio.Event`로 브로드캐스트를 구현해요.
- **기대효과**: 관리자 UX 개선, 장기 실행 작업 진행률 노출, 재시도/취소 트리거 용이.

## 7. 멀티모달 추론 파이프라인 통합
- **배경**: 이미지/영상 모델이 별도 코드로 운용되면 공통 로직(전처리, 후처리, 로깅)이 중복돼요.
- **개선안**: `core/infer/pipeline.py`를 신설해 공통 인터페이스(`BaseInferencePipeline`)를 정의하고, 이미지/영상 파이프라인을 상속 구조로 구성해요. 입력 타입에 따라 자동 라우팅하는 팩토리를 작성해요.
- **기대효과**: 코드 재사용성 증가, 신규 모달(예: 오디오) 추가 시 개발 비용 절감, 테스트 커버리지 확보 용이.

## 8. Explainability 서비스 모듈화
- **배경**: heatmap 생성 로직이 메인 추론 경로와 뒤섞여 있으면 응답 지연이 길어져요.
- **개선안**: `POST /explain/heatmap` 엔드포인트를 비동기 태스크로 분리하고, Grad-CAM/Score-CAM 등 선택 옵션을 설정 파일에서 읽게 해요. 결과물은 S3/MinIO에 저장하고 presigned URL을 반환해요.
- **기대효과**: 메인 추론의 latency 영향 최소화, explainability 파이프라인 재사용성 향상, 스토리지 비용 추적 가능.

## 9. 재학습 루프용 피드백 수집 파이프라인
- **배경**: 오탐/미탐 샘플을 수동으로 수집하면 재학습 속도가 느려요.
- **개선안**: `POST /metrics/ingest`로 들어온 피드백을 Kafka/Redis Stream에 적재하고, `scripts/feedback_consumer.py`에서 주기적으로 S3 버킷에 누적해요. 샘플 메타데이터에는 confidence, 업로드 플랫폼, 사용자 태그를 포함해요.
- **기대효과**: 반자동 재학습 루프 구축, 데이터 드리프트 모니터링, 관리자 대시보드 통계 활용.

## 10. CI/CD 파이프라인에 추론 회귀 테스트 추가
- **배경**: 모델 버전을 교체할 때 기존 API 응답이 달라지면 운영 장애가 발생할 수 있어요.
- **개선안**: `scripts/inference_smoke_test.py`를 작성해 대표 이미지/영상 샘플로 추론 결과를 비교하고, 허용 편차(예: ±0.02) 이상이면 실패하도록 해요. GitHub Actions에서 Docker 이미지 빌드 후 pytest + smoke test를 자동 실행해요.
- **기대효과**: 모델 배포 안전성 향상, 성능 회귀 조기 탐지, 배포 승인 절차 간소화.

